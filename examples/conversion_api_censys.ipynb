{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5889ca-a54f-4780-9841-f108e12f4088",
   "metadata": {},
   "source": [
    "This notebook shows how to use the CensysDatasetManager to convert Censys files into DataFrame abstraction, to enrich and manage this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c220736-ce6b-415d-bfe7-5cc955a3f383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from tlhop.converters import CensysDatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf2db5-3e56-4a58-a0a2-34b2c73d3ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "            .master(\"local[10]\")\\\n",
    "            .config(\"spark.driver.memory\", \"20g\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca936dcb-9966-4059-bea2-edff216b84e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_SNAPSHOT_FOLDER = \"<SNAPSHOT_FOLDER>\"\n",
    "TMP_OUTPUT = \"/home/<USER>/censys.delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34335354-9d6e-4806-be58-3e98b3e8af1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "censys_mgr = CensysDatasetManager(filter_by_contry='Brazil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e63ef-3d51-4518-8b73-7484b2106877",
   "metadata": {},
   "outputs": [],
   "source": [
    "censys_mgr.convert_files(INPUT_SNAPSHOT_FOLDER, TMP_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456b048-2820-423a-a36e-aaa39764b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, to convert, without writing into a new file: \n",
    "# \n",
    "# df = censys_mgr.convert_dump_to_df(INPUT_SNAPSHOT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09088d08-ed5b-4b6c-83ed-1f5c95c7a995",
   "metadata": {},
   "source": [
    "After convert all files, users can access the dataset using Spark native API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04194ab1-eb6b-47e5-9ac4-c09e36404f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(TMP_OUTPUT)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21518b0a-4e27-415c-8f2c-c9606712603c",
   "metadata": {},
   "source": [
    "### Dataset optimization\n",
    "\n",
    "Spark may generate small files over time. Because of that, we expose a method (`optimize_delta`) to optimize the dataset by merging small files into a bigger size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42e028-fff3-402f-82cb-a55812a05926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "censys_mgr.optimize_delta(TMP_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3a8b1-c1d5-4ef8-a92b-5d866a90cec5",
   "metadata": {},
   "source": [
    "Delta format supports time travel. In order to support this feature, older files version are kept inside dataset folder (for instance, it keeps the version before the execution of `optimize_delta` method). When we ensure that older dataset versions are not needed anymore, we can use the `remove_old_delta_versions` method to force a removal of these old versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa084d-4d93-4053-848b-0599a1595478",
   "metadata": {},
   "source": [
    "### Cleaning old versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3d345-abc7-45db-8dc1-8d49bd7110db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "censys_mgr.remove_old_delta_versions(TMP_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b2531-5e5e-4180-8ab9-2f4875e83f37",
   "metadata": {},
   "source": [
    "### Further Delta operations\n",
    "\n",
    "Because we use Delta, further operations are also available using native Delta API. For instance, we can check the complete dataset history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af654fc-0a67-42fa-87c7-2224b4c5eaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, TMP_OUTPUT)\n",
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891369f-8bf8-44e1-9375-117c199e8b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427d105-e0ed-4719-96a4-397320d8af2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
