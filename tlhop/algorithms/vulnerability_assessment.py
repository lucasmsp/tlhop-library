#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as F


from tlhop.datasets import DataSets
from tlhop.converters import ShodanDatasetManager
from tlhop.library import match_cpes_simple
import tlhop.crawlers as crawlers

import os
from datetime import datetime

class ShodanVulnerabilitiesBanners(object):

    _ERROR_MESSAGE_000 = "[ERROR] This algorithm requires an environment variable 'TLHOP_DATASETS_PATH' containing a folder path to be used as storage to all TLHOP datasets."
    _ERROR_MESSAGE_001 = "[ERROR] None active Spark session was found. Please start a new Spark session before use DataSets API."
    _ERROR_MESSAGE_002 = "[ERROR] You must run `compute_general_report` first."
    
    _INFO_MESSAGE_001 = "[INFO] Checking and getting new informations about CVEs from NIST/NVD."
    _INFO_MESSAGE_002 = "[INFO] Checking and getting new informations about EPSS from First EPSS."
    _INFO_MESSAGE_003 = "[INFO] Processing Shodan dump ..."
    _INFO_MESSAGE_004 = "[INFO] Saving data ..."
    _INFO_MESSAGE_005 = "[INFO] Checking and getting new informations about CISA's Known Exploited Vulnerabilities."
    _INFO_MESSAGE_006 = "[INFO] Checking and getting new informations about AS."

    def __init__(self, org_refinement=True, fix_brazilian_cities=True):

        root_path = os.environ.get("TLHOP_DATASETS_PATH", None)
        if not root_path:
            raise Exception(self._ERROR_MESSAGE_000)
        self.root_path = (root_path+"/").replace("//", "/")

        self.spark_session = SparkSession.getActiveSession()
        if not self.spark_session:
            raise Exception(self._ERROR_MESSAGE_001)

        self.fix_brazilian_cities = fix_brazilian_cities
        self.shodan_conversor = ShodanDatasetManager(org_refinement=True, fix_brazilian_cities=fix_brazilian_cities, only_vulns=True)
        self.shodan_conversor.filter_columns = [
            "_shodan", "timestamp", "ip", "ipv6", "ip_str", "org", "isp", "data", "port", "asn", "hostnames", "domains", 
            "location", "os", "device", "devicetype", "vulns", "vulns_verified",'cpe23',  "http"
        ]
        self.ds = DataSets()
        self.crawler = crawlers.FirstEPSS()
        self.result_path = None


    def compute_general_report(self, input_filepath, output_filename, epss_day=None):

        # downloading external datasets
        print(self._INFO_MESSAGE_001)
        nvd = self.ds.read_dataset("NVD_CVE_LIB", check_update=True)\
            .filter(F.col("cvss_score").isNotNull())\
            .withColumn("cvss_rank", F.when(F.col("cvss_version") == 3.1, F.col("cvss_v3.rank")).otherwise(F.col("cvss_v2.rank")))\
            .withColumn("cvss", F.when(F.col("cvss_version") == 3.1, F.to_json("cvss_v3")).otherwise(F.to_json("cvss_v2")))\
            .select("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'cvss', 'cwe', 'cpes')
        
        if not epss_day:
            epss_day = datetime.now().strftime("%Y-%m-%d")

        print(self._INFO_MESSAGE_002)
        epss = self.crawler.download_to_df(epss_day)\
            .select('cve_id', 'epss', 'epss_rank')

        print(self._INFO_MESSAGE_005)
        cisa = self.ds.read_dataset("CISA_EXPLOITS", check_update=True)\
            .withColumn("cisa_info", F.struct(F.col("vendorProject").alias("vendor"), 
                                      F.col("product"), 
                                      F.col("vulnerabilityName").alias("name"), 
                                      F.col("dateAdded").alias("date_added"), 
                                      F.col("shortDescription").alias("description"), 
                                      F.col("knownRansomwareCampaignUse"), 
                                      F.col("cwe")
                                     ))\
            .select("cve_id", "cisa_info")

        external_ds = nvd.join(epss, "cve_id", how="left")\
            .join(cisa, "cve_id", how="left")\
            .withColumn("vulns", F.struct("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'epss', 'epss_rank', "cwe", F.to_json("cpes").alias("cpes"), "cisa_info"))\
            .select("cve_id", "vulns")    

        print(self._INFO_MESSAGE_006)
        asrank = self.ds.read_dataset("AS_RANK_FILE", check_update=True)\
            .select("asn", "as_name", "as_country_iso", "as_country_name", "rank", "clique", "seen", "org_name", "org_country_name", "org_contry_iso", "announcing_prefixes", "announcing_addresses")\
            .withColumn("as_country_name", F.concat_ws(", ", "as_country_name", "as_country_iso"))\
            .withColumn("org_country_name", F.concat_ws(", ", "org_country_name", "org_contry_iso"))\
            .drop("as_country_iso", "org_contry_iso")\
            .withColumn("as", F.struct("*"))\
            .select("asn", "as")

        # converting shodan dump
        print(self._INFO_MESSAGE_003)
        df = self.shodan_conversor.convert_dump_to_df(input_filepath, fast_mode=True)\
            .filter(F.col("location.country_code") == "BR")\
            .filter(F.col("ip").isNotNull())\
            .drop("ip_str", "ip_int", "ipv6")

        df = df.withColumn("city", F.concat_ws(", ", "location.city", "location.region_code"))\
            .select("meta_id", "timestamp", "ip", "org", "org_clean", "isp", "data", "port", 'asn', "hostnames", "domains", 
                    "city", 'location.latitude', 'location.longitude', "os", "device", "devicetype", "vulns_cve", "vulns_verified", 'cpe23',
                    "http",
                   )\
            .tlhop_extension.parser_complex_column("http", "http")\
            .withColumn("http", F.to_json(F.struct(F.col("http.status"), F.col("http.title"), F.col("http.host"), F.col("http.server"), F.col("http.html"))))
            #.tlhop_extension.parser_complex_column("ssl", "ssl")\
            #.withColumn("ssl", F.to_json(F.col("ssl").getField("cert").getField("fingerprint")))\
    
        all_columns = df.columns
        all_columns.remove("vulns_cve")
        all_columns.remove("vulns_verified")

        result = df.withColumn("cve_id", F.explode("vulns_cve"))\
            .join(external_ds, "cve_id")\
            .withColumn("verified", F.array_contains("vulns_verified", "cve_id"))\
            .withColumn("vulns", F.struct(F.col("vulns.*"), F.col("verified").alias("verified")))\
            .groupby(all_columns).agg(F.collect_list("vulns").alias("vulns"), F.first("vulns_verified").alias("vulns_verified"))\
            .withColumn("vulns_scores", F.struct(F.col("vulns.cve_id").alias("cve_id"), 
                                                 F.col("vulns.epss").alias("epss"), 
                                                 F.col("vulns.cvss_score").alias("cvss_score")))

        result = result.join(asrank, "asn", "left").drop("asn")
            
        result.coalesce(3*10)\
          .write\
          .format("delta")\
          .option("userMetadata", input_filepath)\
          .option("mergeSchema", "true")\
          .mode("overwrite")\
          .save(output_filename)

        self.result_path = output_filename

        print("OK!")


    def gen_extra_query1(self):
        """
        Summarization of the number of vulnerable IPs and organizations by EPSS scores.
        """

        if self.result_path:
            v1 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .select("ip", "org_clean", "as.asn", "port", F.explode("vulns").alias("vulns"))\
                .select("ip", "org_clean", "asn", "port", "vulns.cve_id", "vulns.epss_rank")\
                .groupby("epss_rank").agg(F.countDistinct("cve_id").alias("n_cves"),
                                         F.countDistinct("org_clean").alias("n_orgs"),
                                         F.countDistinct("ip").alias("n_ips"),
                                         F.countDistinct("port").alias("n_port"),
                                         F.countDistinct("asn").alias("n_as"))\
                .orderBy("epss_rank")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v1

    def gen_extra_query2(self):
        if self.result_path:
            
            w = Window.partitionBy("org_clean", 'ip', 'cpe_product', 'cpe_version').orderBy(F.desc("epss"))

            v2 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .filter(F.col("cpe23").isNotNull())\
                .select("ip", "org_clean", "cpe23", F.explode("vulns").alias("vulns"))\
                .select("ip", "org_clean", "cpe23", 'vulns.cve_id', 'vulns.epss', 'vulns.epss_rank','vulns.cvss_score', 'vulns.cvss_rank', 'vulns.cvss_version', "vulns.cpes")\
                .withColumn("cpes",  F.col("cpes"))\
                .withColumn("cpe", match_cpes_simple('cpe23', 'cpes'))\
                .select("*", "cpe.*")\
                .drop("cpes", "cpe23", 'cpe')\
                .filter(F.col("cpe_product").isNotNull())\
                .withColumn("n_row", F.row_number().over(w))\
                .withColumn("cve_all", F.collect_set("cve_id").over(w))\
                .filter("n_row == 1")\
                .select("org_clean", 'ip', 'cpe_product', 'cpe_version', 'cve_id', 'epss', 'epss_rank','cvss_score','cvss_rank', 'cvss_version', 'cve_all')
    
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v2
        
    def gen_extra_query3(self):
        """
        Informations about all vulnerabilities
        """
        if self.result_path:
            v3 = self.spark_session.read.format("delta").load(self.result_path)\
                    .select("ip", "org_clean", "as.asn", "port", F.explode("vulns").alias("vulns"))\
                    .select("ip", "org_clean", "asn", "port", "vulns.*")\
                    .drop("cpes")
    
            keys = [k for k in v3.columns if k not in ["ip", "org_clean", "asn", "port"]]
            
            v3 = v3.groupby(*keys)\
                .agg(F.countDistinct("org_clean").alias("n_orgs"), 
                     F.countDistinct("ip").alias("n_ips"),
                     F.countDistinct("port").alias("n_port"),
                     F.countDistinct("asn").alias("n_as"))\
                .orderBy(F.desc("epss"), F.desc("n_orgs"), F.desc("cvss_score"))
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v3

    def gen_extra_query4(self):
        """
        Informations about AS
        """
        if self.result_path:
            v4 = self.spark_session.read.format("delta").load(self.result_path)\
                .select("ip", "org_clean", "city", "as.*", F.explode("vulns").alias("vulns"))\
                  .select("*","vulns.cve_id", "vulns.cvss_score", "vulns.cvss_rank", "vulns.epss", "vulns.epss_rank", 
                        F.col("vulns.cisa_info.name").alias("cisa_name"), F.col("vulns.cisa_info.date_added").alias("cisa_added"))\
                .withColumn("cisa_cve_id", F.when(F.col("cisa_added").isNotNull(), F.col("cve_id")))\
                .drop("vulns", "cisa_added")\
            
            v4.groupby("asn","as_name", "as_country_name", "org_name", "org_country_name", "rank", "clique", "seen", "announcing_prefixes", "announcing_addresses")\
                .agg(
                    F.countDistinct("ip").alias("n_ips"),
                    F.countDistinct("org_clean").alias("n_orgs"),
                    F.collect_set("city").alias("cities"),
                    F.collect_set("cve_id").alias("cve_list"),
                    F.max("cvss_score").alias("max_cvss"),
                    F.avg("cvss_score").alias("avg_cvss"),
                    F.min("cvss_score").alias("min_cvss"),
                    F.max("epss").alias("max_epss"),
                    F.avg("epss").alias("avg_epss"),
                    F.min("epss").alias("min_epss"),
                    F.collect_set("cisa_name").alias("cisa_vuln_names"),
                    F.collect_set("cisa_cve_id").alias("cisa_vulns")
                )
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v4
