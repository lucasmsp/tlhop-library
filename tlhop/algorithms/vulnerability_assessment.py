#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as F


from tlhop.datasets import DataSets
from tlhop.converters import ShodanDatasetManager
from tlhop.library import match_cpes_simple
import tlhop.crawlers as crawlers

import os
from datetime import datetime

class ShodanVulnerabilitiesBanners(object):

    _ERROR_MESSAGE_000 = "[ERROR] This algorithm requires an environment variable 'TLHOP_DATASETS_PATH' containing a folder path to be used as storage to all TLHOP datasets."
    _ERROR_MESSAGE_001 = "[ERROR] None active Spark session was found. Please start a new Spark session before use DataSets API."
    _ERROR_MESSAGE_002 = "[ERROR] You must run `compute_general_report` first."
    
    _INFO_MESSAGE_001 = "[INFO] Checking and getting new informations about CVEs from NIST/NVD."
    _INFO_MESSAGE_002 = "[INFO] Checking and getting new informations about EPSS from First EPSS."
    _INFO_MESSAGE_003 = "[INFO] Processing Shodan dump ..."
    _INFO_MESSAGE_004 = "[INFO] Saving data ..."

    def __init__(self, org_refinement=True, fix_brazilian_cities=True):

        root_path = os.environ.get("TLHOP_DATASETS_PATH", None)
        if not root_path:
            raise Exception(self._ERROR_MESSAGE_000)
        self.root_path = (root_path+"/").replace("//", "/")

        self.spark_session = SparkSession.getActiveSession()
        if not self.spark_session:
            raise Exception(self._ERROR_MESSAGE_001)

        self.fix_brazilian_cities = fix_brazilian_cities
        self.shodan_conversor = ShodanDatasetManager(org_refinement=True, fix_brazilian_cities=fix_brazilian_cities, only_vulns=True)
        self.shodan_conversor.filter_columns = [
            "_shodan", "timestamp", "ip_str", "org", "isp", "data", "port", "hostnames", "domains", 
            "location", "os", "device", "devicetype", "vulns", 'cpe23',  "http"
        ]
        self.ds = DataSets()
        self.crawler = crawlers.FirstEPSS()
        self.result_path = None


    def compute_general_report(self, input_filepath, output_filename, check_nvd=True, epss_day=None):

        if check_nvd:
            print(self._INFO_MESSAGE_001)
        nvd = self.ds.read_dataset("NVD_CVE_LIB", check_update=check_nvd)\
            .filter(F.col("cvss_score").isNotNull())\
            .withColumn("cvss_rank", F.when(F.col("cvss_version") == 3.1, F.col("cvss_v3.rank")).otherwise(F.col("cvss_v2.rank")))\
            .withColumn("cvss", F.when(F.col("cvss_version") == 3.1, F.to_json("cvss_v3")).otherwise(F.to_json("cvss_v2")))\
            .select("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'cvss', 'cwe', 'cpes')
        
        if not epss_day:
            epss_day = datetime.now().strftime("%Y-%m-%d")

        print(self._INFO_MESSAGE_002)
        epss = self.crawler.download_to_df(epss_day)\
            .select('cve_id', 'epss', 'epss_rank')

        cves_epss = nvd.join(epss, "cve_id")\
            .withColumn("vulns", F.struct("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'epss', 'epss_rank', "cwe", F.to_json("cpes").alias("cpes")))\
            .select("cve_id", "vulns")

        print(self._INFO_MESSAGE_003)

        df = self.shodan_conversor.convert_dump_to_df(input_filepath, fast_mode=True)\
            .filter(F.col("ip_str").isNotNull())\
            .filter(F.col("location.country_code") == "BR")\
            .select("meta_id", "timestamp", "ip_str", "org", "org_clean", "isp", "data", "port", "hostnames", "domains", 
                    "location.city", "location.region_code", 'location.latitude', 'location.longitude', "os", "device", "devicetype", "vulns_cve", 'cpe23',
                    "http",
                   )\
            .tlhop_extension.parser_complex_column("http", "http")\
            .withColumn("http", F.to_json(F.struct(F.col("http.status"), F.col("http.title"), F.col("http.host"), F.col("http.server"), F.col("http.html"))))
            #.tlhop_extension.parser_complex_column("ssl", "ssl")\
            #.withColumn("ssl", F.to_json(F.col("ssl").getField("cert").getField("fingerprint")))\
    
        all_columns = df.columns
        all_columns.remove("vulns_cve")

        result = df.withColumn("cve_id", F.explode("vulns_cve"))\
            .join(cves_epss, "cve_id")\
            .groupby(all_columns).agg(F.collect_list("vulns").alias("vulns"))\
            .withColumn("vulns_scores", F.struct(F.col("vulns.cve_id").alias("cve_id"), 
                                                           F.col("vulns.epss").alias("epss"), 
                                                           F.col("vulns.cvss_score").alias("cvss_score")))
            

        result.write\
          .format("delta")\
          .option("userMetadata", input_filepath)\
          .option("mergeSchema", "true")\
          .mode("overwrite")\
          .save(output_filename)

        self.result_path = output_filename

        print("OK!")


    def gen_extra_query1(self):
        """
        Summarization of the number of vulnerable IPs and organizations by EPSS scores.
        """

        if self.result_path:
            v1 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .select("ip_str", "org_clean", F.explode("vulns").alias("vulns"))\
                .select("ip_str", "org_clean", "vulns.cve_id", "vulns.epss_rank")\
                .groupby("epss_rank").agg(F.countDistinct("cve_id").alias("n_cves"),
                                         F.countDistinct("org_clean").alias("n_orgs"),
                                         F.countDistinct("ip_str").alias("n_ips"))\
                .orderBy("epss_rank")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v1

    def gen_extra_query2(self):
        if self.result_path:
            
            w = Window.partitionBy("org_clean", 'ip_str', 'cpe_product', 'cpe_version').orderBy(F.desc("epss"))

            v2 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .filter(F.col("cpe23").isNotNull())\
                .select("ip_str", "org_clean", "cpe23", F.explode("vulns").alias("vulns"))\
                .select("ip_str", "org_clean", "cpe23", 'vulns.cve_id', 'vulns.epss', 'vulns.epss_rank','vulns.cvss_score', 'vulns.cvss_rank', 'vulns.cvss_version', "vulns.cpes")\
                .withColumn("cpes",  F.col("cpes"))\
                .withColumn("cpe", match_cpes_simple('cpe23', 'cpes'))\
                .select("*", "cpe.*")\
                .drop("cpes", "cpe23", 'cpe')\
                .filter(F.col("cpe_product").isNotNull())\
                .withColumn("n_row", F.row_number().over(w))\
                .withColumn("others_cves", F.array_except(F.collect_set("cve_id").over(w), F.array(F.col("cve_id"))))\
                .filter("n_row == 1")\
                .select("org_clean", 'ip_str', 'cpe_product', 'cpe_version', 'cve_id', 'epss', 'epss_rank','cvss_score','cvss_rank', 'cvss_version', 'others_cves')
    
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v2
        
    def gen_extra_query3(self):
        """
        Informations about all vulnerabilities
        """
        if self.result_path:
            v3 = self.spark_session.read.format("delta").load(self.result_path)\
                .select("ip_str", "org_clean", F.explode("vulns").alias("vulns"))\
                .groupby( "vulns.cve_id", 'vulns.cvss_score', 'vulns.cvss_rank', 'vulns.cvss_version',"vulns.epss","vulns.epss_rank")\
                .agg(F.countDistinct("org_clean").alias("n_orgs"), 
                     F.countDistinct("ip_str").alias("n_ips"))\
                .orderBy(F.desc("epss"), F.desc("n_orgs"), F.desc("cvss_score"))
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v3
        
