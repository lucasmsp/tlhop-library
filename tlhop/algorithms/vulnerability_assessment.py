#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as F


from tlhop.datasets import DataSets
from tlhop.converters import ShodanDatasetManager
from tlhop.library import match_cpes_simple
import tlhop.crawlers as crawlers

import re2 as re
import os
from datetime import datetime

class ShodanVulnerabilitiesBanners(object):

    _ERROR_MESSAGE_000 = "[ERROR] This algorithm requires an environment variable 'TLHOP_DATASETS_PATH' containing a folder path to be used as storage to all TLHOP datasets."
    _ERROR_MESSAGE_001 = "[ERROR] None active Spark session was found. Please start a new Spark session before use DataSets API."
    _ERROR_MESSAGE_002 = "[ERROR] You must run `compute_general_report` first."
    _ERROR_MESSAGE_003 = "[ERROR] Error in computation!"
    
    _INFO_MESSAGE_001 = "[INFO] Checking and getting new informations about CVEs from NIST/NVD."
    _INFO_MESSAGE_002 = "[INFO] Checking and getting new informations about EPSS from First EPSS."
    _INFO_MESSAGE_003 = "[INFO] Processing Shodan dump ..."
    _INFO_MESSAGE_004 = "[INFO] Saving data ..."
    _INFO_MESSAGE_005 = "[INFO] Checking and getting new informations about CISA's Known Exploited Vulnerabilities."
    _INFO_MESSAGE_006 = "[INFO] Checking and getting new informations about AS."

    def __init__(self, input_filepath, output_filename, epss_day=None, org_refinement=True, fix_brazilian_cities=True):

        root_path = os.environ.get("TLHOP_DATASETS_PATH", None)
        if not root_path:
            raise Exception(self._ERROR_MESSAGE_000)
        self.root_path = (root_path+"/").replace("//", "/")

        self.spark_session = SparkSession.getActiveSession()
        if not self.spark_session:
            raise Exception(self._ERROR_MESSAGE_001)

        self.fix_brazilian_cities = fix_brazilian_cities
        self.shodan_conversor = ShodanDatasetManager(org_refinement=True, fix_brazilian_cities=fix_brazilian_cities, only_vulns=True)
        self.shodan_conversor.filter_columns = [
            "_shodan", "timestamp", "ip", "ipv6", "ip_str", "org", "isp", "data", "port", "asn", "hostnames", "domains", 
            "location", "os", "device", "devicetype", "vulns", "vulns_verified",'cpe23',  "http"
        ]
        self.ds = DataSets()
        self.crawler = crawlers.FirstEPSS()
        self.result_path = None
        self.input_filepath = input_filepath
        self.output_filename = output_filename
        self.number_of_cores = 10
        self.epss_day = epss_day


    def compute_general_report(self):
        # downloading external datasets
        print(self._INFO_MESSAGE_001)
        nvd = self.ds.read_dataset("NVD_CVE_LIB", check_update=True)\
            .filter(F.col("cvss_score").isNotNull())\
            .withColumn("cvss_rank", F.when(F.col("cvss_version") == 3.1, F.col("cvss_v3.rank")).otherwise(F.col("cvss_v2.rank")))\
            .withColumn("cvss", F.when(F.col("cvss_version") == 3.1, F.to_json("cvss_v3")).otherwise(F.to_json("cvss_v2")))\
            .select(F.col("cve_id").alias("vulns_cve_id"), 
                    F.round(F.col("cvss_score"), 1).alias("vulns_cvss_score"),
                    F.col("cvss_version").alias("vulns_cvss_version"),
                    F.col("cvss_rank").alias("vulns_cvss_rank"),
                    F.col("cvss").alias("vulns_cvss"),
                    F.col("cwe").alias("vulns_cwe"),
                    F.to_json("cpes").alias("vulns_cpes"))

        
        if not self.epss_day:
            self.epss_day = datetime.now().strftime("%Y-%m-%d")

        print(self._INFO_MESSAGE_002)
        epss = self.crawler.download_to_df(self.epss_day)\
            .select(F.col("cve_id").alias("vulns_cve_id"), 
                    F.round(F.col("epss"), 2).alias("vulns_epss"), 
                    F.col("epss_rank").alias("vulns_epss_rank"))
            

        print(self._INFO_MESSAGE_005)
        cisa = self.ds.read_dataset("CISA_EXPLOITS", check_update=True)\
            .select(F.col("cve_id").alias("vulns_cve_id"),
                    F.col("vendorProject").alias("vulns_cisa_vendor"), 
                    F.col("product").alias("vulns_cisa_product"), 
                    F.col("vulnerabilityName").alias("vulns_cisa_name"), 
                    F.col("dateAdded").alias("vulns_cisa_date_added"), 
                    F.col("shortDescription").alias("vulns_cisa_description"), 
                    F.col("knownRansomwareCampaignUse").alias("vulns_cisa_knownRansomwareCampaignUse"))

        vulns_cols = ["vulns_cve_id", "vulns_cvss_score", "vulns_cvss_version", "vulns_cvss_rank", 'vulns_epss', 'vulns_epss_rank', "vulns_cwe", "vulns_cpes", 
                      "vulns_cisa_vendor", 'vulns_cisa_product', 'vulns_cisa_name', 'vulns_cisa_date_added', 'vulns_cisa_description', 'vulns_cisa_knownRansomwareCampaignUse']
        
        external_ds = nvd.join(epss, "vulns_cve_id", how="left")\
            .join(cisa, "vulns_cve_id", how="left")\
            .select(*vulns_cols)
        external_ds = replaces_nulls(external_ds)        

        print(self._INFO_MESSAGE_006)
        asrank = self.ds.read_dataset("AS_RANK_FILE", check_update=True)\
            .select("asn", "as_name", "as_country_iso", "as_country_name", "rank", "clique", "seen", "org_name", "org_country_name", "org_country_iso", "announcing_prefixes", "announcing_addresses")\
            .withColumn("as_country_name", F.concat_ws(", ", "as_country_name", "as_country_iso"))\
            .withColumn("org_country_name", F.concat_ws(", ", "org_country_name", "org_country_iso"))\
            .select("asn", "as_name", "as_country_name", 
                    F.col("rank").alias("as_rank"), 
                    F.col("clique").alias("as_clique"), 
                    F.col("seen").alias("as_seen"), 
                    F.col("org_name").alias("as_org_name"), 
                    F.col("org_country_name").alias("as_org_country_name"), 
                    F.col("announcing_prefixes").alias('as_announcing_prefixes'), 
                    F.col("announcing_addresses").alias('as_announcing_addresses'))

        # converting shodan dump
        print(self._INFO_MESSAGE_003)
        df = self.shodan_conversor.convert_dump_to_df(self.input_filepath, fast_mode=True)\
            .filter(F.col("location.country_code") == "BR")\
            .filter(F.col("ip").isNotNull())\
            .withColumn("data", F.substring('data', 1, 500))\
            .drop("ip_str", "ip_int", "ipv6")\
            .repartition(3 * self.number_of_cores)

        df = df.withColumn("city", F.concat_ws(", ", "location.city", "location.region_code"))\
            .select("meta_id", F.col("shodan.id").alias("shodan_id"), "timestamp", "ip", "org", "org_clean", "isp", "data", "port", 'asn', "hostnames", 
                    "domains", "city", 'location.latitude', 'location.longitude', "os", "device", "devicetype", "vulns_cve", "vulns_verified", 'cpe23',
                    "http",
                   )\
            .tlhop_extension.parser_complex_column("http", "http")\
            .withColumn("http", F.to_json(F.struct(F.col("http.status"), F.col("http.title"), F.col("http.host"), F.col("http.server"), F.col("http.html"))))\
            .withColumn("hostnames", F.array_join(F.col("hostnames"), "\n\n"))\
            .withColumn("domains", F.array_join(F.col("domains"), "\n\n"))\
            .withColumn("servers", format_data("data"))
            #.tlhop_extension.parser_complex_column("ssl", "ssl")\
            #.withColumn("ssl", F.to_json(F.col("ssl").getField("cert").getField("fingerprint")))\
    
        all_columns = df.columns
        all_columns.remove("vulns_cve")
        all_columns.remove("vulns_verified")

        vulns_cols.append("vulns_cve_verified")
        aggs = []
        for c in vulns_cols:
            aggs.append(F.collect_list(c).alias(c))

        result = df.withColumn("vulns_cve_id", F.explode("vulns_cve"))\
            .join(external_ds, "vulns_cve_id")\
            .withColumn("vulns_cve_verified", F.array_contains("vulns_verified", "vulns_cve_id"))\
            .groupby(all_columns).agg(*aggs)

        result = result.join(asrank, "asn", "left")
        
        report_output = self.output_filename + ".delta"

        try:
            result.coalesce(2 * self.number_of_cores)\
              .write\
              .format("delta")\
              .option("userMetadata", self.input_filepath)\
              .option("mergeSchema", "true")\
              .mode("overwrite")\
              .save(report_output)
        
        except Exception as e:
            print(e)
            raise(self._ERROR_MESSAGE_003)

        self.result_path = report_output

        self.spark_session.catalog.clearCache()
        print("OK!")

    def _set_spark_config(self, state):
        if state == "begin":
            self.spark_session.conf.set("spark.sql.parquet.columnarReaderBatchSize", "512")
        elif state == 'end':
            self.spark_session.conf.set("spark.sql.parquet.columnarReaderBatchSize", "4096")


    def gen_query_summary(self, write=True):
        """
        Summarization of the number of vulnerable IPs and organizations by EPSS scores.
        """
        v1 = None
        
        if self.result_path:
            
            v1 = self.spark_session.read.format("delta").load(self.result_path)\
                .repartition(2 * self.number_of_cores)\
                .filter(F.col("org_clean").isNotNull())\
                .withColumn("new", F.arrays_zip("vulns_cve_id", "vulns_epss_rank"))\
                .select("ip", "org_clean", "asn", "port", F.explode("new").alias("new"))\
                .groupby("new.vulns_epss_rank")\
                    .agg(F.countDistinct("new.vulns_cve_id").alias("n_cves"),
                         F.countDistinct("org_clean").alias("n_orgs"),
                         F.countDistinct("ip").alias("n_ips"),
                         F.countDistinct("port").alias("n_port"),
                         F.countDistinct("asn").alias("n_as")
                        )\
                .orderBy("vulns_epss_rank")
            
            if write:
                try:
                    v1.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-summary.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v1

    def gen_query_orgs(self, write=True):
        """
        Summarization of the number of vulnerable IPs and organizations by EPSS scores.
        """
        v2 = None
        
        if self.result_path:
            self._set_spark_config('begin')
            
            vulns_cols = ['vulns_cve_id', 'vulns_epss', 'vulns_epss_rank','vulns_cvss_score', 'vulns_cvss_rank', 'vulns_cvss_version', "vulns_cpes"]

            w = Window.partitionBy("org_clean")
            w2 = Window.partitionBy("org_clean").orderBy(F.desc("vulns_epss"))
            
            v2 = self.spark_session.read.format("delta").load(self.result_path)\
                .repartition(3 * self.number_of_cores)\
                .filter(F.col("org_clean").isNotNull())\
                .filter(F.col("cpe23").isNotNull())\
                .select("ip", "org_clean", "cpe23", F.explode(F.arrays_zip(*vulns_cols)).alias("vulns"))\
                .select("ip", "org_clean", "cpe23", 'vulns.vulns_cve_id', 'vulns.vulns_epss', 'vulns.vulns_epss_rank','vulns.vulns_cvss_score',
                        'vulns.vulns_cvss_version', 'vulns.vulns_cvss_rank', "vulns.vulns_cpes")\
                .withColumn("vulns_cpe", match_cpes_simple('cpe23', 'vulns_cpes'))\
                .select("*", "vulns_cpe.*")\
                .withColumn("vulns_cve_list", F.collect_set(F.col('vulns_cve_id')).over(w))\
                .withColumn("n_vulns", F.size('vulns_cve_list'))\
                .withColumn("n_ips", F.size(F.collect_set(F.col('ip')).over(w)))\
                .withColumn("n_products", F.size(F.collect_set(F.col('cpe_product')).over(w)))\
                .withColumn("n_row", F.row_number().over(w2))\
                .filter("n_row == 1")\
                .select("org_clean", 'vulns_cve_id', "vulns_epss", "vulns_epss_rank", F.col("vulns_cvss_score").alias("vulns_cvss"), 
                        'vulns_cvss_rank', 'vulns_cvss_version', 'cpe_product','n_vulns', 'n_ips', 'n_products', 'vulns_cve_list')

            if write:
                try:
                    v2.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-orgs.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
            self._set_spark_config('end')
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v2
        
    def gen_query_ips(self, write=True):
        """

        """
        v2 = None
        
        if self.result_path:
            self._set_spark_config('begin')
            
            vulns_cols = ['vulns_cve_id', 'vulns_epss', 'vulns_epss_rank','vulns_cvss_score', 'vulns_cvss_rank', 'vulns_cvss_version', "vulns_cpes"]
            
            w = Window.partitionBy("org_clean", 'ip').orderBy(F.desc("vulns_epss"))

            v2 = self.spark_session.read.format("delta").load(self.result_path)\
                .repartition(3 * self.number_of_cores)\
                .filter(F.col("org_clean").isNotNull())\
                .filter(F.col("cpe23").isNotNull())\
                .select("ip", "org_clean", "cpe23", F.col("vulns_cve_id").alias("vulns_cve_all"), F.explode(F.arrays_zip(*vulns_cols)).alias("vulns"))\
                .select("ip", "org_clean", "cpe23", 'vulns.vulns_cve_id', 'vulns.vulns_epss', 'vulns.vulns_epss_rank','vulns.vulns_cvss_score', 
                        'vulns.vulns_cvss_rank', 'vulns.vulns_cvss_version', "vulns.vulns_cpes", F.col("vulns_cve_all").alias("vulns_cve_list"))\
                .withColumn("vulns_cpe", match_cpes_simple('cpe23', 'vulns_cpes'))\
                .select("*", "vulns_cpe.*")\
                .drop("vulns_cpes", "cpe23", 'vulns_cpe')\
                .withColumn("n_row", F.row_number().over(w))\
                .filter("n_row == 1")\
                .select("org_clean", 'ip', 'cpe_product', 'cpe_version', 'vulns_cve_id', 'vulns_epss', 'vulns_epss_rank',
                        F.col('vulns_cvss_score').alias("vulns_cvss"), 'vulns_cvss_rank', 'vulns_cvss_version', 'vulns_cve_list')

            if write:
                try:
                    v2.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-ips.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
            self._set_spark_config('end')
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v2
        
    def gen_query_vulns(self, write=True):
        """
        Informations about all vulnerabilities
        """
        v3 = None
        
        if self.result_path:
            self._set_spark_config('begin')

            vulns_cols = ["vulns_cve_id", "vulns_cvss_score", "vulns_cvss_version", "vulns_cvss_rank", 'vulns_epss', 'vulns_epss_rank', "vulns_cwe", "vulns_cpes", 
                    "vulns_cisa_vendor", 'vulns_cisa_product', 'vulns_cisa_name', 'vulns_cisa_date_added', 'vulns_cisa_description', 'vulns_cisa_knownRansomwareCampaignUse']
            
            v3 = self.spark_session.read.format("delta").load(self.result_path)\
                    .repartition(3 * self.number_of_cores)\
                    .withColumn("vulns", F.arrays_zip(*vulns_cols))\
                    .select("ip", "org_clean", "asn", "port", F.explode("vulns").alias("vulns"))\
                    .select("ip", "org_clean", "asn", "port", "vulns.*")\
                    .withColumnRenamed("vulns_cvss_score", "vulns_cvss")\
                    .drop("cpes")
    
            keys = [k for k in v3.columns if k not in ["ip", "org_clean", "asn", "port", "vulns_cve_id"]]

            # workaround to handle large number of columns
            values = [F.first(c).alias(c) for c in keys] + \
                [
                    F.countDistinct("org_clean").alias("n_orgs"), 
                    F.countDistinct("ip").alias("n_ips"),
                    F.countDistinct("port").alias("n_port"),
                    F.countDistinct("asn").alias("n_as")
                ]
            
            v3 = v3.groupby('vulns_cve_id')\
                .agg(*values)\
                .orderBy(F.desc("vulns_epss"), F.desc("n_orgs"), F.desc("vulns_cvss"))

            if write:
                try:
                    v3.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-vulns.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
        else:
            print(self._ERROR_MESSAGE_002)
            
        self._set_spark_config('end')
        return v3

    def gen_query_as(self, write=True):
        """
        Informations about AS
        """
        v4 = None
        
        if self.result_path:
            self._set_spark_config('begin')
            
            vulns_cols = ["vulns_cve_id", "vulns_cvss_score", "vulns_cvss_rank", 'vulns_epss', 
                          'vulns_epss_rank', 'vulns_cisa_name', 'vulns_cisa_date_added']
            
            v4 = self.spark_session.read.format("delta").load(self.result_path)\
                .repartition(2 * self.number_of_cores)\
                .select("ip", "org_clean", "city", "asn","as_name", "as_country_name", "as_org_name", "as_org_country_name", 
                        "as_rank", "as_clique", "as_seen", "as_announcing_prefixes", "as_announcing_addresses", F.explode(F.arrays_zip(*vulns_cols)).alias("vulns"))\
                .select("*","vulns.*")\
                .withColumn("cisa_cve_id", F.when(F.col("vulns_cisa_date_added").isNotNull(), F.col("vulns_cve_id")))\
                .groupby("asn", "as_name", "as_country_name", "as_org_name", "as_org_country_name", "as_rank", "as_clique", "as_seen", "as_announcing_prefixes", "as_announcing_addresses")\
                .agg(
                    F.countDistinct("ip").alias("n_ips"),
                    F.countDistinct("org_clean").alias("n_orgs"),
                    F.collect_set("city").alias("city_list"),
                    F.collect_set("vulns_cve_id").alias("vulns_cve_list"),
                    F.max("vulns_cvss_score").alias("vulns_cvss_max"),
                    F.round(F.avg("vulns_cvss_score"), 1).alias("vulns_cvss_avg"),
                    F.min("vulns_cvss_score").alias("vulns_cvss_min"),
                    F.max("vulns_epss").alias("vulns_epss_max"),
                    F.round(F.avg("vulns_epss"), 2).alias("vulns_epss_avg"),
                    F.min("vulns_epss").alias("vulns_epss_min"),
                    F.collect_set("cisa_cve_id").alias("vulns_cisa_list")
                )

            if write:
                try:
                    v4.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-as.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
        else:
            print(self._ERROR_MESSAGE_002)
            
        self._set_spark_config('end')
        return v4


    def gen_query_ports(self, write=True):
        """
        Informations about Ports
        """
        v5 = None
        
        if self.result_path:
            self._set_spark_config('begin')
            
            vulns_cols = ['vulns_cve_id', 'vulns_epss', 'vulns_cvss_score', "vulns_cpes", "vulns_cwe", "vulns_cisa_vendor", "vulns_cisa_date_added", 'vulns_cisa_product', 'vulns_cisa_knownRansomwareCampaignUse']

            v5 = self.spark_session.read.format("delta").load(self.result_path)\
                .repartition(2 * self.number_of_cores)\
                .select(*vulns_cols, "ip", "port", 'cpe23')\
                .withColumn("n_vulns", F.size("vulns_cve_id"))\
                .withColumn("vulns", F.explode(F.arrays_zip(*vulns_cols)))\
                .select("ip","port", "n_vulns", 'cpe23', "vulns.*")\
                .withColumn("vulns_cpe", match_cpes_simple('cpe23', 'vulns_cpes'))\
                .withColumn("cpe_product", F.when(F.col("vulns_cpe.cpe_product").isNotNull(), F.col("vulns_cpe.cpe_product")).otherwise(F.lower(F.concat_ws(":", "vulns_cisa_vendor", "vulns_cisa_product"))))\
                .select("ip","port", "n_vulns", "cpe_product", 'vulns_cve_id', 'vulns_epss', 'vulns_cvss_score', 'vulns_cisa_knownRansomwareCampaignUse', 'vulns_cisa_date_added')\
                .withColumn("vulns_cisa_knownRansomwareCampaignUse", F.when(F.col("vulns_cisa_knownRansomwareCampaignUse") == "Known", F.lit(1)).otherwise(F.lit(0)))\
                .withColumn("vulns_cisa_date_added", F.when(F.col("vulns_cisa_date_added").isNotNull(), F.lit(1)).otherwise(F.lit(0)))\
                .groupby("port")\
                .agg(
                    F.collect_set("vulns_cve_id").alias("cve_list"),
                    F.collect_set("cpe_product").alias("product_list"),
                    F.max("vulns_epss").alias("vulns_epss_max"),
                    F.round(F.avg("vulns_epss"), 2).alias("vulns_epss_avg"),
                    F.min("vulns_epss").alias("vulns_epss_min"),
                    F.max("vulns_cvss_score").alias("vulns_cvss_score_max"),
                    F.min("vulns_cvss_score").alias("vulns_cvss_score_min"),
                    F.round(F.avg("vulns_cvss_score"), 1).alias("vulns_cvss_score_avg"),
                    F.sum("vulns_cisa_date_added").alias("n_vulns_cisa"),
                    F.sum("vulns_cisa_knownRansomwareCampaignUse").alias("n_vulns_cisa_ransomware")
                )\
                .withColumn("n_vulns", F.size("cve_list"))\
                .withColumn("n_products", F.size("product_list"))

            if write:
                try:
                    v5.coalesce(1).write\
                        .format("delta")\
                        .mode("overwrite")\
                        .option("userMetadata", self.input_filepath)\
                        .option("mergeSchema", "true")\
                        .save(self.output_filename + "-ports.delta")
                except Exception as e:
                    print(e)
                    raise(self._ERROR_MESSAGE_003)
            
            print("OK!")
        else:
            print(self._ERROR_MESSAGE_002)
            
        self._set_spark_config('end')
        return v5


@F.udf
def format_data(raw):

    if not raw:
        raw = ""

    space_index = raw.find(' ')
    if space_index != -1:
        truncate_data = raw[0:space_index]
    else:
        truncate_data = raw
    
    truncate_data = truncate_data.split("\n")[0]
    
    text = f"**{truncate_data}**"

    match1 = re.search('Server: [^\r\n]+', raw)
    if match1:
        server = match1.group()
        text += f"\n\n{server}"

    match2 = re.search('SMB Version: [^\r\n]+', raw)
    if match2:
        version = match2.group()
        text += f"\n\n{version}"

    match3 = re.search('Date: [^\r\n]+', raw)
    if match3:
        date_match = match3.group()
        text += f"\n\n{date_match}"   

    return text


def replaces_nulls(external_ds):

    dtypes = dict(external_ds.dtypes)
    replaces = {}
    for c in dtypes:
        if dtypes.get(c, 'string') == "string":
            replaces[c] = "-"
        elif dtypes.get(c, 'string') in ["int", "double"]:
            replaces[c] = -1
            
    return external_ds.na.fill(replaces)
