#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as F


from tlhop.datasets import DataSets
from tlhop.converters import ShodanDatasetManager
from tlhop.library import match_cpes_simple
import tlhop.crawlers as crawlers

import os
from datetime import datetime

class ShodanVulnerabilitiesBanners(object):

    _ERROR_MESSAGE_000 = "[ERROR] This algorithm requires an environment variable 'TLHOP_DATASETS_PATH' containing a folder path to be used as storage to all TLHOP datasets."
    _ERROR_MESSAGE_001 = "[ERROR] None active Spark session was found. Please start a new Spark session before use DataSets API."
    _ERROR_MESSAGE_002 = "[ERROR] You must run `compute_general_report` first."
    
    _INFO_MESSAGE_001 = "[INFO] Checking and getting new informations about CVEs from NIST/NVD."
    _INFO_MESSAGE_002 = "[INFO] Checking and getting new informations about EPSS from First EPSS."
    _INFO_MESSAGE_003 = "[INFO] Processing Shodan dump ..."
    _INFO_MESSAGE_004 = "[INFO] Saving data ..."
    _INFO_MESSAGE_005 = "[INFO] Checking and getting new informations about CISA's Known Exploited Vulnerabilities."
    _INFO_MESSAGE_006 = "[INFO] Checking and getting new informations about AS."

    def __init__(self, input_filepath, output_filename, epss_day=None, org_refinement=True, fix_brazilian_cities=True):

        root_path = os.environ.get("TLHOP_DATASETS_PATH", None)
        if not root_path:
            raise Exception(self._ERROR_MESSAGE_000)
        self.root_path = (root_path+"/").replace("//", "/")

        self.spark_session = SparkSession.getActiveSession()
        if not self.spark_session:
            raise Exception(self._ERROR_MESSAGE_001)

        self.fix_brazilian_cities = fix_brazilian_cities
        self.shodan_conversor = ShodanDatasetManager(org_refinement=True, fix_brazilian_cities=fix_brazilian_cities, only_vulns=True)
        self.shodan_conversor.filter_columns = [
            "_shodan", "timestamp", "ip", "ipv6", "ip_str", "org", "isp", "data", "port", "asn", "hostnames", "domains", 
            "location", "os", "device", "devicetype", "vulns", "vulns_verified",'cpe23',  "http"
        ]
        self.ds = DataSets()
        self.crawler = crawlers.FirstEPSS()
        self.result_path = None
        self.input_filepath = input_filepath
        self.output_filename = output_filename
        self.number_of_cores = 10
        self.epss_day = epss_day


    def compute_general_report(self):
        # downloading external datasets
        print(self._INFO_MESSAGE_001)
        nvd = self.ds.read_dataset("NVD_CVE_LIB", check_update=True)\
            .filter(F.col("cvss_score").isNotNull())\
            .withColumn("cvss_rank", F.when(F.col("cvss_version") == 3.1, F.col("cvss_v3.rank")).otherwise(F.col("cvss_v2.rank")))\
            .withColumn("cvss", F.when(F.col("cvss_version") == 3.1, F.to_json("cvss_v3")).otherwise(F.to_json("cvss_v2")))\
            .select("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'cvss', 'cwe', 'cpes')
        
        if not self.epss_day:
            self.epss_day = datetime.now().strftime("%Y-%m-%d")

        print(self._INFO_MESSAGE_002)
        epss = self.crawler.download_to_df(self.epss_day)\
            .select('cve_id', 'epss', 'epss_rank')

        print(self._INFO_MESSAGE_005)
        cisa = self.ds.read_dataset("CISA_EXPLOITS", check_update=True)\
            .select("cve_id", F.col("vendorProject").alias("cisa_vendor"), 
                              F.col("product").alias("cisa_product"), 
                              F.col("vulnerabilityName").alias("cisa_name"), 
                              F.col("dateAdded").alias("cisa_date_added"), 
                              F.col("shortDescription").alias("cisa_description"), 
                              F.col("knownRansomwareCampaignUse").alias("cisa_knownRansomwareCampaignUse"))

        external_ds = nvd.join(epss, "cve_id", how="left")\
            .join(cisa, "cve_id", how="left")\
            .withColumn("vulns", F.struct("cve_id", "cvss_score", "cvss_version", "cvss_rank", 'epss', 'epss_rank', "cwe", F.to_json("cpes").alias("cpes"), 
                                          "cisa_vendor", 'cisa_product', 'cisa_name', 'cisa_date_added', 'cisa_description', 'cisa_knownRansomwareCampaignUse'))\
            .select("cve_id", "vulns")    

        print(self._INFO_MESSAGE_006)
        asrank = self.ds.read_dataset("AS_RANK_FILE", check_update=True)\
            .select("asn", "as_name", "as_country_iso", "as_country_name", "rank", "clique", "seen", "org_name", "org_country_name", "org_country_iso", "announcing_prefixes", "announcing_addresses")\
            .withColumn("as_country_name", F.concat_ws(", ", "as_country_name", "as_country_iso"))\
            .withColumn("org_country_name", F.concat_ws(", ", "org_country_name", "org_country_iso"))\
            .drop("as_country_iso", "org_country_iso")\
            .withColumn("as", F.struct("*"))\
            .select("asn", "as")

        # converting shodan dump
        print(self._INFO_MESSAGE_003)
        df = self.shodan_conversor.convert_dump_to_df(self.input_filepath, fast_mode=True)\
            .filter(F.col("location.country_code") == "BR")\
            .filter(F.col("ip").isNotNull())\
            .drop("ip_str", "ip_int", "ipv6")

        df = df.withColumn("city", F.concat_ws(", ", "location.city", "location.region_code"))\
            .select("meta_id", "timestamp", "ip", "org", "org_clean", "isp", "data", "port", 'asn', "hostnames", "domains", 
                    "city", 'location.latitude', 'location.longitude', "os", "device", "devicetype", "vulns_cve", "vulns_verified", 'cpe23',
                    "http",
                   )\
            .tlhop_extension.parser_complex_column("http", "http")\
            .withColumn("http", F.to_json(F.struct(F.col("http.status"), F.col("http.title"), F.col("http.host"), F.col("http.server"), F.col("http.html"))))
            #.tlhop_extension.parser_complex_column("ssl", "ssl")\
            #.withColumn("ssl", F.to_json(F.col("ssl").getField("cert").getField("fingerprint")))\
    
        all_columns = df.columns
        all_columns.remove("vulns_cve")
        all_columns.remove("vulns_verified")

        result = df.withColumn("cve_id", F.explode("vulns_cve"))\
            .join(external_ds, "cve_id")\
            .withColumn("verified", F.array_contains("vulns_verified", "cve_id"))\
            .withColumn("vulns", F.struct(F.col("vulns.*"), F.col("verified").alias("verified")))\
            .groupby(all_columns).agg(F.collect_list("vulns").alias("vulns"), F.first("vulns_verified").alias("vulns_verified"))\
            .withColumn("vulns_scores", F.struct(F.col("vulns.cve_id").alias("cve_id"), 
                                                 F.col("vulns.epss").alias("epss"), 
                                                 F.col("vulns.cvss_score").alias("cvss_score")))

        result = result.join(asrank, "asn", "left")\
            .drop("asn")\
            .select("*", 
                    "as.asn", 
                    "as.as_name", 
                    "as.as_country_name", 
                    F.col("as.rank").alias("as_rank"), 
                    F.col("as.clique").alias("as_clique"), 
                    F.col("as.seen").alias("as_seen"), 
                    F.col("as.org_name").alias("as_org_name"), 
                    F.col("as.org_country_name").alias("as_org_country_name"), 
                    F.col("as.announcing_prefixes").alias('as_announcing_prefixes'), 
                    F.col("as.announcing_addresses").alias('as_announcing_addresses')
                   ).drop("as")
        
        report_output = self.output_filename + ".delta"
        result.coalesce(3 * self.number_of_cores)\
          .write\
          .format("delta")\
          .option("userMetadata", self.input_filepath)\
          .option("mergeSchema", "true")\
          .mode("overwrite")\
          .save(report_output)

        self.result_path = report_output

        self.spark_session.catalog.clearCache()
        print("OK!")


    def gen_extra_query1(self):
        """
        Summarization of the number of vulnerable IPs and organizations by EPSS scores.
        """

        if self.result_path:
            v1 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .select("ip", "org_clean", "asn", "port", F.explode("vulns").alias("vulns"))\
                .select("ip", "org_clean", "asn", "port", "vulns.cve_id", "vulns.epss_rank")\
                .groupby("epss_rank").agg(F.countDistinct("cve_id").alias("n_cves"),
                                         F.countDistinct("org_clean").alias("n_orgs"),
                                         F.countDistinct("ip").alias("n_ips"),
                                         F.countDistinct("port").alias("n_port"),
                                         F.countDistinct("asn").alias("n_as"))\
                .orderBy("epss_rank")

            v1.coalesce(1).write\
                .format("delta")\
                .mode("overwrite")\
                .option("userMetadata", self.input_filepath)\
                .option("mergeSchema", "true")\
                .save(self.output_filename + "-view1.delta")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v1

    def gen_extra_query2(self):
        if self.result_path:
            
            w = Window.partitionBy("org_clean", 'ip', 'cpe_product', 'cpe_version').orderBy(F.desc("epss"))

            v2 = self.spark_session.read.format("delta").load(self.result_path)\
                .filter(F.col("org_clean").isNotNull())\
                .filter(F.col("cpe23").isNotNull())\
                .select("ip", "org_clean", "cpe23", F.explode("vulns").alias("vulns"))\
                .select("ip", "org_clean", "cpe23", 'vulns.cve_id', 'vulns.epss', 'vulns.epss_rank','vulns.cvss_score', 'vulns.cvss_rank', 'vulns.cvss_version', "vulns.cpes")\
                .withColumn("cpes",  F.col("cpes"))\
                .withColumn("cpe", match_cpes_simple('cpe23', 'cpes'))\
                .select("*", "cpe.*")\
                .drop("cpes", "cpe23", 'cpe')\
                .filter(F.col("cpe_product").isNotNull())\
                .withColumn("n_row", F.row_number().over(w))\
                .withColumn("cve_all", F.collect_set("cve_id").over(w))\
                .filter("n_row == 1")\
                .select("org_clean", 'ip', 'cpe_product', 'cpe_version', 'cve_id', 'epss', 'epss_rank','cvss_score','cvss_rank', 'cvss_version', 'cve_all')

            v2.coalesce(1).write\
                .format("delta")\
                .mode("overwrite")\
                .option("userMetadata", self.input_filepath)\
                .option("mergeSchema", "true")\
                .save(self.output_filename + "-view2.delta")
    
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v2
        
    def gen_extra_query3(self):
        """
        Informations about all vulnerabilities
        """
        if self.result_path:
            v3 = self.spark_session.read.format("delta").load(self.result_path)\
                    .repartition(self.number_of_cores * 3)\
                    .select("ip", "org_clean", "asn", "port", F.explode("vulns").alias("vulns"))\
                    .select("ip", "org_clean", "asn", "port", "vulns.*")\
                    .drop("cpes")
    
            keys = [k for k in v3.columns if k not in ["ip", "org_clean", "asn", "port", "cve_id"]]

            # workaround to handle large number of columns
            values = [F.first(c).alias(c) for c in keys] + \
                [
                    F.countDistinct("org_clean").alias("n_orgs"), 
                    F.countDistinct("ip").alias("n_ips"),
                    F.countDistinct("port").alias("n_port"),
                    F.countDistinct("asn").alias("n_as")
                ]
            
            v3 = v3.groupby('cve_id')\
                .agg(*values)\
                .orderBy(F.desc("epss"), F.desc("n_orgs"), F.desc("cvss_score"))

            v3.coalesce(4).write\
                .format("delta")\
                .mode("overwrite")\
                .option("userMetadata", self.input_filepath)\
                .option("mergeSchema", "true")\
                .save(self.output_filename + "-view3.delta")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v3

    def gen_extra_query4(self):
        """
        Informations about AS
        """
        if self.result_path:
            v4 = self.spark_session.read.format("delta").load(self.result_path)\
                .select("ip", "org_clean", "city", "asn","as_name", "as_country_name", "as_org_name", "as_org_country_name", 
                        "as_rank", "as_clique", "as_seen", "as_announcing_prefixes", "as_announcing_addresses", F.explode("vulns").alias("vulns"))\
                  .select("*","vulns.cve_id", "vulns.cvss_score", "vulns.cvss_rank", "vulns.epss", "vulns.epss_rank", 
                        F.col("vulns.cisa_name").alias("cisa_name"), F.col("vulns.cisa_date_added").alias("cisa_added"))\
                .withColumn("cisa_cve_id", F.when(F.col("cisa_added").isNotNull(), F.col("cve_id")))\
                .drop("vulns", "cisa_added")\
            
            v4.groupby("asn","as_name", "as_country_name", "as_org_name", "as_org_country_name", "as_rank", "as_clique", "as_seen", "as_announcing_prefixes", "as_announcing_addresses")\
                .agg(
                    F.countDistinct("ip").alias("n_ips"),
                    F.countDistinct("org_clean").alias("n_orgs"),
                    F.collect_set("city").alias("cities"),
                    F.collect_set("cve_id").alias("cve_list"),
                    F.max("cvss_score").alias("max_cvss"),
                    F.avg("cvss_score").alias("avg_cvss"),
                    F.min("cvss_score").alias("min_cvss"),
                    F.max("epss").alias("max_epss"),
                    F.avg("epss").alias("avg_epss"),
                    F.min("epss").alias("min_epss"),
                    F.collect_set("cisa_name").alias("cisa_vuln_names"),
                    F.collect_set("cisa_cve_id").alias("cisa_vulns")
                )

            v4.coalesce(2).write\
                .format("delta")\
                .mode("overwrite")\
                .option("userMetadata", self.input_filepath)\
                .option("mergeSchema", "true")\
                .save(self.output_filename + "-view4.delta")
        else:
            print(self._ERROR_MESSAGE_002)
        
        return v4
